---
title: "02_assessment (IN PROGRESS)"
description: |
    Analyzing the results of the assessment process helps uncover the reasons behind disagreements found among reviewers, so that documents such as the AGILE Guidelines and the Assessemnt Protocol can be updated accordingly.

#execute:
#  echo: false
format: 
  html:
    toc: true
    embed-resources: true  # Self-contained
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
---

## Introduction

The majority of the work happened during the assessment process: evaluation of the papers, consolidation of the assessments, and **synthesis of the rich feedback** as well as interpretation of the findings. Here, we retrieve data items required to explore feedback from reviewers during the assessment process. Individual scores of conceptual paper, data, methods, results (computational environment is not useful at the reviewer level), and notes from the two reviewers as well as **types of disagreement** between reviewers are relevant here.

```{r}
#| label: load-packages
#| message: false

library(here)
library(tidyverse)
library(dplyr)
library(scales)
library(stringr)
library(gt)
library(ggplot2)
```

```{r}
#| label: functions
#| eval: false
#| echo: false

# Taken from https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-reproducibility-assessment.Rmd

criteriaBarplot = function(category, main, colour) {
  cat <- enquo(category)
  ggplot2::ggplot(data = paper_evaluation_wo_conceptual,
                aes(!!cat),
                show.legend = FALSE) +
    ggplot2::geom_bar(fill = colours[colour], color = "black") +
    ggplot2::ggtitle(main) +
    ggplot2::xlab("Level") +
    ggplot2::xlim(level_names) +
    ggplot2::ylab("") +
    ggplot2::scale_y_continuous(breaks = breaks,
                                limits = range(breaks)) # +
    #ggthemes::theme_tufte(base_size = 8) + theme(axis.ticks.x = element_blank())
}
```

```{r}
#| label: load-data
#| message: false

assessment <- 
  readr::read_csv(here::here("data-clean", "all-data.csv"))

assessment <- 
  assessment |>
  dplyr::select(conf, paper, year, 
                rev1, rev1_cp, rev1_data, rev1_methods, rev1_results, rev1_notes,
                rev2, rev2_cp, rev2_data, rev2_methods, rev2_results, rev2_notes,
                disagr_type, disagr_id) |>
  dplyr::arrange(conf, year)

  


```

Selected/relevant columns are: `{r} stringr::str_flatten_comma(names(assessment))`. More details [here](https://github.com/nuest/reproducible-research-giscience-longitudinal-study/tree/main/data-clean).

```{r}
#| label: basic-counts
#| message: false

n_papers <- nrow(assessment)
n_papers_agile <- 
  assessment |>
  count(conf) |>
  filter(conf == "agile") |>
  pull(n)


n_papers_giscience <- 
  assessment |>
  count(conf) |>
  filter(conf == "giscience") |>
  pull(n)

#n_cp <- nrow(filter(assessment, consolidated_cp == TRUE))
#p_cp <- n_cp / n_papers

```

## Types of disagreements

### Overview: all disagreement types together

These are the types of disagreements (`disagr_type`) between the individual reviewers' scores.

-   **no disagreement**: Reviewers' scores are the same in all dimensions (conceptual paper, data, methods, results)

-   **borderline conceptual paper**: Disagreement are due to a different understanding whether a paper is completely conceptual; some reviewers may see a paper that only has demonstrative, synthetic example data as not “data-based” or empirical and exclude it from the assessment.

-   **annotation inconsistencies**: Disagreements are due to errors in the level/score selected given the written notes of either reviewer. No matter the dimension where inconsistencies are.

-   **uncertain assessment (according to protocol assessment)**: either reviewer wrote that they are unsure about level x or y, so we could just change that to the level matching the other reviewer.

-   **significant disagreement**: when reviewers' notes (or lack of notes) cannot explain differences in assessment and conversation to get an agreement is necessary and/or checking again the paper is required. **TODO: Replace `major disagreement` by `significant disagreement` in dataset**

```{r}
#| label: tbl-disagr-types
#| tbl-cap: "Distribution of disagreement types"

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("No grouping (N={n_papers})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = md("**Disagreement type**"),
    n = md("**#**"),
    p = md("**%**")
  )

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::slice_max(order_by = p, n = 1) |>
  pull(p) -> p_disagr_top1

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::filter(disagr_type != "no disagreement") |> # Exclude agreement, of course
  dplyr::slice_max(order_by = p, n = 2) |>
  dplyr::summarize(sum = sum(p)) |>
  dplyr::pull(sum) -> p_disagr_top2

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::filter(disagr_type == "no disagreement") |> # Only total agreement
  dplyr::pull(p) -> p_no_disagr

```


First, the reviewers made the same assessment in **`{r} scales::label_percent(accuracy = 0.1)(p_no_disagr)`** of the papers. Reviewers disagreed in more than 70% of the total number of papers (N=`{r} n_papers`)!

Second, @tbl-disagr-types also shows that, after excluding `no disagreement` (well, it means perfect match between reviewers!), the two most common disagreement types cover (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top2)`**). Uncertainty in the application/interpretation of the assessment criteria covers a large proportion of the disagreements (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top1)`**), so there is room to improve and clarify aspects of the *Assessment Protocol* document to avoid slightly different interpretations of how the evaluation criteria have to be applied.



```{r}
#| label: tbl-disagr-types-conf
#| tbl-cap: "Distribution of disagreement types per conference: AGILE vs GIScience"

assessment |>
  dplyr::group_by(conf) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(conf, desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("Grouped by conference (N={n_papers})") 
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::summary_rows(
    groups = everything(),
    columns = n,
    fns = list(
      total = "sum"
    )
  ) |>
  gt::cols_label(
    disagr_type = "",
    n = md("**#**"),
    p = md("**%**")
  )

```

In general, @tbl-disagr-types-conf shows no remarkable differences between disagreement types per conference. Let's focus on the three intermediate disagreement types which have different percentages. AGILE papers present greater percentage of `major disagreement`, while GIScience papers have more papers assigned to `borderline conceptual paper` and `no disagreement`. Speculating a bit, we can say that GIScience papers tend to be more theoretical than AGILE articles, which are typically classified as applied research or application papers. Following this line of argument, GIScience papers may be much easier to evaluate than AGILE papers, in which greater uncertainty may arise when scoring the level of reproducibility for data, methods, and results. That is, more theoretical papers often use fewer computational methods and data artifacts than applied research papers, so it makes sense that `borderline conceptual paper` and `no disagreement` are common disagreement types in theory-oriented papers. Therefore, the nature of research (I mean, the number of theory-oriented papers vs the number of application-oriented papers) between the two conferences could be one of the factor that explain these small, but still significant, differences in `no disagreement`, `major disagreement`, and `borderline conceptual paper`.

```{r}
#| label: tbl-disagr-types-conf-year
#| tbl-cap: "Distribution of disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

disagr_agile <-
  assessment |>
  dplyr::filter(conf =="agile") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))


disagr_agile |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )


disagr_giscience <-
  assessment |>
  dplyr::filter(conf =="giscience") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))

disagr_giscience |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )

```

```{r}
#| label: fig-disagr-types-conf-year
#| fig-cap: "Distribution of disagreement types per conference and year (plots)"
#| fig-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2
#| fig-column: page


disagrLinePlot <- function(data, main) {
  ggplot2::ggplot(data, 
                  aes(x = year, y = p, color = disagr_type)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point(size = 3) +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +  # Format y-axis as %
  ggplot2::guides(
      color = guide_legend(ncol = 1, position = "right")) +
  ggplot2::labs(
    title = main,
    x = "Year",
    y = "Proportion [%]",
    color = "Disagreement Type"
  ) +
  ggplot2::theme_minimal()
  
}
  

fig_agile <- disagrLinePlot(disagr_agile, "Proportion of Disagreement Types by Year (AGILE)")
fig_giscience <- disagrLinePlot(disagr_giscience, "Proportion of Disagreement Types by Year (GIScience)")

fig_agile
fig_giscience


# Taken from https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-reproducibility-assessment.Rmd
#fig_barplot <- patchwork::wrap_plots(
#  ncol = 5,
#  criteriaBarplot(`input data`,    main = "Input data",    colour = 1),
#  criteriaBarplot(`method/analysis/processing`,
#                  main = "Methods/Analysis/\nProcessing",  colour = 3),
#  criteriaBarplot(`computational environment`,
#)

#fig_barplot
```

Looking into the data over years, @tbl-disagr-types-conf-year breaks disagreement types per conference and year. @fig-disagr-types-conf-year represents the same data as line charts.

-   `borderline conceptual paper` disagreements follow similar trend in both conferences. It is almost residual in recent years (1 paper per year/conference). It was different in the first year of the series (AGILE 2017, GIScience 2016) where more disagreements of this type occurred.

-   `annotation inconsistencies` is residual.

-   `no disagreement`, `uncertain assessment` and `major disagreement` appear to be somewhat related. In general, when `no disagreement` increases, the other two decrease. And viceversa.

-   `no disagreement` tends to decrease over time in both conferences, suggesting that agreement between reviewers in assessing the level of reproducibility in the first years of the series (AGILE 2017, GIScience 2026, GIScience 2018) was *much easier* than in recent years. Why? Possible reasons:

    -   Authors have increasingly adopted best practices for describing or including other research resources used in their articles, which were often omitted in articles in earlier conferences.
    -   The number of computational papers at recent conferences tends to be higher than in previous years.

-   `uncertain assessment` and `major disagreement`. Together, it is hard to interpret them. Well, a pattern is visible over the last three conference editions: From a maximum peak (AGILE 2022, GIScience 2020), percentage of `uncertain assessment` decreases while that of `major disagreement` increases for both conferences. Yet, the increase of `major disagrement` is greater in the last GIScience conference. Why: Perhaps the lack of explicit reproducibility guidelines for GIScience authors may have a factor as the number of computational papers in GIScience goes up.

### Only `no disagreement`, `uncertain assessment` and `major disagreement`

```{r}
#| label: tbl-top3-disagr-types-conf-year
#| tbl-cap: "Distribution of 3-top disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

disagr_agile |>
  dplyr::filter(disagr_type %in% c("no disagreement", "uncertain assessment", "major disagreement")) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = c("no disagreement", "uncertain assessment", "major disagreement")
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )


disagr_giscience |>
  dplyr::filter(disagr_type %in% c("no disagreement", "uncertain assessment", "major disagreement")) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = c("no disagreement", "uncertain assessment", "major disagreement")
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )
```

@tbl-top3-disagr-types-conf-year "zooms" in @tbl-disagr-types-conf-year to show in detail the 3-top disagreement types.

-   it's pretty clear that `no disagreement` has steadily declined since the beginning of the series, reaching its lowest percentage in the final year of the series for both conferences. However this common pattern develops differently for each conference: `no dissagrement` halved in AGILE 2018 and remained stable at this trend until AGILE 2024.

-   For the same year of AGILE papers, the highest percentages (in red) of `uncertain assessment` coincide with the lowest percentages (blue) of `major disagreement` (in blue). And the opposite is true too. However, this pattern, although present, is not repeated as clearly in the GIScience articles.

## Looking at individdual types of disagreements

We explore the scores (data, methods, and results) of the two reviewers for each type of disagreement, excluding `no disagrement`, `borderline conceptual paper`, and `annotation inconsistencies`.

### Type: `uncertain assessment`

```{r}
#| label: disagr-uncertain

n_disagr_uncertain <- 
  assessment |>
  filter(disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)

n_disagr_uncertain_agile <-
  assessment |>
  filter(conf == "agile", disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)

n_disagr_uncertain_giscience <-
  assessment |>
  filter(conf == "giscience", disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)


disagr_uncertain <- 
  assessment |>
  dplyr::select(-paper, -rev1, -rev2, -rev1_cp, -rev2_cp, -rev1_notes, -rev2_notes) |>
  dplyr::filter(disagr_type == "uncertain assessment") |>
  dplyr::mutate(disagr_dat = (rev1_data != rev2_data)) |>
  dplyr::mutate(disagr_met = (rev1_methods != rev2_methods)) |>
  dplyr::mutate(disagr_res = (rev1_results != rev2_results)) 

```

```{r}
#| label: tbl-disagr-uncertain-conf
#| tbl-cap: "Distribution of 'Uncertain disagreement'"

tbl_disagr_uncertain <-
  bind_cols(
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_dat) |>
    dplyr::count(name = "n_dat") |>
    dplyr::ungroup(disagr_dat) |>
    dplyr::mutate(p_dat = n_dat / sum(n_dat)) |>
    dplyr::ungroup(),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_met) |>
    dplyr::count(name = "n_met") |>
    dplyr::ungroup(disagr_met) |>
    dplyr::mutate(p_met = n_met / sum(n_met)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_res) |>
    dplyr::count(name = "n_res") |>
    dplyr::ungroup(disagr_res) |>
    dplyr::mutate(p_res = n_res / sum(n_res)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf)
  )

tbl_disagr_uncertain |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("Uncertain disagreement (N={n_disagr_uncertain})"),
    subtitle = md("Applied to *Data*, *Methods* and *Results* scores (of the two reviewers).<br>The lowest disagreements are in cyan, the highest disagreements in red.")
  ) |>
  gt::tab_spanner(
    label = md("**Data**"),
    columns = ends_with("_dat"),
  ) |>
  gt::tab_spanner(
    label = md("**Methods**"),
    columns = ends_with("_met"),
  ) |>
  gt::tab_spanner(
    label = md("**Results**"),
    columns = ends_with("_res"),
  ) |>
  gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = 
      cells_body(
        rows = disagr_dat == TRUE)
  ) |>
  gt::tab_style(
    style = cell_fill(color = "lightcyan"),
    locations = list(
      cells_body(
        columns = p_met,
        rows = p_met <= 0.3),
      cells_body(
        columns = p_res,
        rows = p_res <= 0.2)
    )
  ) |>
  gt::tab_style(
    style = list(
      cell_fill(color = "indianred"),
      cell_text(color = "white")
      ),
    locations = cells_body(
        columns = p_dat,
        rows = p_dat >= 0.6
    )
  ) |>
  gt::fmt_percent(
    columns = c(p_dat, p_met, p_res),
    decimals = 1
  ) |>
  gt::cols_label(
    conf = md("**Conf Series**"),
    disagr_dat = md("*Disagreement?*"),
    disagr_met = md("*Disagreement?*"),
    disagr_res = md("*Disagreement?*"))

```

What we observe in @tbl-disagr-uncertain-conf (rows in **bold** only): 

- AGILE: 
  - Lowest proportion of `uncertain assessment` disagreements is in **Methods**, under 30%. 
  - Highest proportion of `uncertain assessment` disagreements is in **Data**, close to 65%.
  - From higher to lower proportion of disagreements: Data -> Results -> Methods
  
- GIScience:
  - Lowest proportion of `uncertain assessment` disagreements is in **Results**, just 20%. 
  - Highest proportion of `uncertain assessment` disagreements is in **Data**, just 80%.
  - From higher to lower proportion of disagreements: Data -> Methods -> Results 



In @fig-disagr-uncertain-heatmap, we look now at the individual scores (UDAI levels) for each of the dimensions (data, methods, results), considering AGILE (top row) and GIScience (bottom row).

```{r}
#| label: fig-disagr-uncertain-heatmap
#| fig-cap: "Counts UDAI score for 'uncertain disagreement' between reviewers (heatmap): AGILE vs GIScience"
#| fig-subcap: 
#|     - "AGILE: Data"
#|     - "AGILE: Methods"
#|     - "AGILE: Results"
#|     - "GIScience: Data"
#|     - "GIScience: Methods"
#|     - "GIScience: Results"
#| layout: [[30, 30, 30], [30, 30, 30]]
#| fig-column: page

# TODO: https://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

udao_levels <-  c("U", "D", "A", "O")

heatmap_agile_dat <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_data, rev2_data) |>
    dplyr::mutate(rev1_data = factor(rev1_data, levels = udao_levels)) |>
    dplyr::mutate(rev2_data = factor(rev2_data, levels = udao_levels)) |>
    dplyr::group_by(rev1_data,rev2_data) %>%
    dplyr::tally()


heatmap_agile_met <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_methods, rev2_methods) |>
    dplyr::mutate(rev1_methods = factor(rev1_methods, levels = udao_levels)) |>
    dplyr::mutate(rev2_methods  = factor(rev2_methods, levels = udao_levels)) |>
    dplyr::group_by(rev1_methods,rev2_methods) %>%
    dplyr::tally()


heatmap_agile_res <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_results, rev2_results) |>
    dplyr::mutate(rev1_results = factor(rev1_results, levels = udao_levels)) |>
    dplyr::mutate(rev2_results = factor(rev2_results, levels = udao_levels)) |>
    dplyr::group_by(rev1_results,rev2_results) %>%
    dplyr::tally()


heatmap_giscience_dat <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_data, rev2_data) |>
    dplyr::mutate(rev1_data = factor(rev1_data, levels = udao_levels)) |>
    dplyr::mutate(rev2_data = factor(rev2_data, levels = udao_levels)) |>
    dplyr::group_by(rev1_data,rev2_data) %>%
    dplyr::tally()


heatmap_giscience_met <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_methods, rev2_methods) |>
    dplyr::mutate(rev1_methods = factor(rev1_methods, levels = udao_levels)) |>
    dplyr::mutate(rev2_methods  = factor(rev2_methods, levels = udao_levels)) |>
    dplyr::group_by(rev1_methods,rev2_methods) %>%
    dplyr::tally()


heatmap_giscience_res <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_results, rev2_results) |>
    dplyr::mutate(rev1_results = factor(rev1_results, levels = udao_levels)) |>
    dplyr::mutate(rev2_results = factor(rev2_results, levels = udao_levels)) |>
    dplyr::group_by(rev1_results,rev2_results) %>%
    dplyr::tally()


heatmap_agile_dat |>
  ggplot2::ggplot(aes(x = rev1_data, y = rev2_data, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Data disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

heatmap_agile_met |>
  ggplot2::ggplot(aes(x = rev1_methods, y = rev2_methods, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Methods disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_agile_res |>
  ggplot2::ggplot(aes(x = rev1_results, y = rev2_results, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Results disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_giscience_dat |>
  ggplot2::ggplot(aes(x = rev1_data, y = rev2_data, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Data disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

heatmap_giscience_met |>
  ggplot2::ggplot(aes(x = rev1_methods, y = rev2_methods, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Methods disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_giscience_res |>
  ggplot2::ggplot(aes(x = rev1_results, y = rev2_results, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Results disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

```

  

### Type: `major disagreement` (to be replaced by `significant disagreement`)


```{r}
#| label: disagr-significant

n_disagr_significant <- 
  assessment |>
  dplyr::filter(disagr_type == "major disagreement") |>
  dplyr::count() |>
  dplyr::pull(n)

n_disagr_significant_agile <-
  assessment |>
  dplyr::filter(conf == "agile", disagr_type == "major disagreement") |>
  dplyr::count() |>
  dplyr::pull(n)

n_disagr_significant_giscience <-
  assessment |>
  dplyr::filter(conf == "giscience", disagr_type == "major disagreement") |>
  dplyr::count() |>
  dplyr::pull(n)


disagr_significant <- 
  assessment |>
  dplyr::select(-paper, -rev1, -rev2, -rev1_cp, -rev2_cp, -rev1_notes, -rev2_notes) |>
  dplyr::filter(disagr_type == "major disagreement") |>
  dplyr::mutate(disagr_dat = (rev1_data != rev2_data)) |>
  dplyr::mutate(disagr_met = (rev1_methods != rev2_methods)) |>
  dplyr::mutate(disagr_res = (rev1_results != rev2_results)) 


```


```{r}
#| label: tbl-disagr-significant-conf
#| tbl-cap: "Distribution of 'major disagreement'"

tbl_disagr_significant <-
  bind_cols(
    disagr_significant |>
    dplyr::group_by(conf, disagr_dat) |>
    dplyr::count(name = "n_dat") |>
    dplyr::ungroup(disagr_dat) |>
    dplyr::mutate(p_dat = n_dat / sum(n_dat)) |>
    dplyr::ungroup(),
    
    disagr_significant |>
    dplyr::group_by(conf, disagr_met) |>
    dplyr::count(name = "n_met") |>
    dplyr::ungroup(disagr_met) |>
    dplyr::mutate(p_met = n_met / sum(n_met)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf),
    
    disagr_significant |>
    dplyr::group_by(conf, disagr_res) |>
    dplyr::count(name = "n_res") |>
    dplyr::ungroup(disagr_res) |>
    dplyr::mutate(p_res = n_res / sum(n_res)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf)
  )

tbl_disagr_significant |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("Major disagreement (N={n_disagr_significant})"),
    subtitle = md("Applied to *Data*, *Methods* and *Results* scores (of the two reviewers).<br>The lowest disagreements are in cyan, the highest disagreements in red.")
  ) |>
  gt::tab_spanner(
    label = md("**Data**"),
    columns = ends_with("_dat"),
  ) |>
  gt::tab_spanner(
    label = md("**Methods**"),
    columns = ends_with("_met"),
  ) |>
  gt::tab_spanner(
    label = md("**Results**"),
    columns = ends_with("_res"),
  ) |>
  gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = 
      cells_body(
        rows = disagr_dat == TRUE)
  ) |>
  gt::tab_style(
    style = list(
      cell_fill(color = "indianred"),
      cell_text(color = "white")
      ),
    locations = cells_body(
        columns = p_dat,
        rows = p_dat >= 0.7
    )
  ) |>
  gt::fmt_percent(
    columns = c(p_dat, p_met, p_res),
    decimals = 1
  ) |>
  gt::cols_label(
    conf = md("**Conf Series**"),
    disagr_dat = md("*Disagreement?*"),
    disagr_met = md("*Disagreement?*"),
    disagr_res = md("*Disagreement?*"))

```

What we observe in @tbl-disagr-significant-conf (rows in **bold** only): 

- AGILE: 
  - Lowest proportion of `major disagreement` is in **Results**. 
  - Highest proportion of `major disagreement` is in **Data**. 
  - From higher to lower proportion of disagreements: Data -> Methods -> Results
  
- GIScience:
  - Lowest proportion of `major disagreement` is in **Methods and Results**, same proportion. 
  - Highest proportion of `uncertain disagreement` is in **Data**.
  - From higher to lower proportion of disagreements: Data -> Methods/Results 


## Remarks
 
The exploratory analysis above discerns some observations (unsure if we can term them as *patterns*) about the disagreements found between reviewers while evaluating the level of reproducibility of AGILE/GIScience papers. The following observations should be considered simply as factual information (which can be converted into recommendations) to improve the clarity of the AGILE Guidelines and the Assessment Protocol.

- Reviewers disagreed in more than 70% of the total number of papers (N=`{r} n_papers`)! See @tbl-disagr-types

- On the positive side, **`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top2)`** of disagreements where due to slightly different interpretations of the evaluation criteria, suggesting that improving the Assessment Protocol could drastically reduce it. On the contrary, `significant disagreement` (almost 20%) denotes that reviewers had not enough information on the assessed paper to obtain a similar score. That is, determining the root causes of disagreement is difficult and, in most cases, is due to several factors. The most relevant could be a lack of detail in the original paper, along with some imprecise instructions in the Assessment Protocol.
 
- Similar proportions of disagreement types per conference - see @tbl-disagr-types-conf.

- See comments above regarding @tbl-disagr-types-conf-year and @fig-disagr-types-conf-year, where disagreement types are divided by conference and year. 
 
- See comments above regarding @tbl-top3-disagr-types-conf-year. 

- Regardless of the conference, the data dimension accounts for the largest proportion of `uncertain assessment` disagreements when analyzing individual scores (see @tbl-disagr-uncertain-conf). That is, the reviewers disagreed primarily on the data dimension. Therefore, although the disagreements discussed (`uncertain assessment`) stem from slightly subjective interpretations of the assessment protocol, the data dimension remains the controversial one. **Therefore**, efforts to improve guidelines on how to clearly evaluate (by reviewers/readers) and unambiguously describe (by authors) input data are crucial.
 
- Regardless of the conference, the data dimension accounts for the largest proportion of `major disagreement` when analyzing individual scores (see @tbl-disagr-significant-conf). 


