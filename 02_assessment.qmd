---
title: "02_assessment (IN PROGRESS)"
description: |
    Analyzing the results of the assessment process helps uncover the reasons behind disagreements found among reviewers, so that documents such as the AGILE Guidelines and the Assessemnt Protocol can be updated accordingly.

#execute:
#  echo: false
format: 
  html:
    toc: true
    embed-resources: true  # Self-contained
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
---


## Introduction 

The majority of the work happened during the assessment process: evaluation of the papers, consolidation of the assessments, and **synthesis of the rich feedback** as well as interpretation of the findings. Here, we retrieve data items required to explore feedback from reviewers during the assessment process. Individual scores of conceptual paper, data, methods, results (computational environment is not useful at the reviewer level), and notes from the two reviewers as well as **types of disagreement** between reviewers are relevant here. 


```{r}
#| label: load-packages
#| message: false

library(here)
library(tidyverse)
library(dplyr)
library(scales)
library(stringr)
library(gt)
library(ggplot2)
```


```{r}
#| label: functions
#| eval: false
#| echo: false

# Taken from https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-reproducibility-assessment.Rmd

criteriaBarplot = function(category, main, colour) {
  cat <- enquo(category)
  ggplot2::ggplot(data = paper_evaluation_wo_conceptual,
                aes(!!cat),
                show.legend = FALSE) +
    ggplot2::geom_bar(fill = colours[colour], color = "black") +
    ggplot2::ggtitle(main) +
    ggplot2::xlab("Level") +
    ggplot2::xlim(level_names) +
    ggplot2::ylab("") +
    ggplot2::scale_y_continuous(breaks = breaks,
                                limits = range(breaks)) # +
    #ggthemes::theme_tufte(base_size = 8) + theme(axis.ticks.x = element_blank())
}
```




```{r}
#| label: load-data
#| message: false

assessment <- 
  readr::read_csv(here::here("data-clean", "all-data.csv"))

assessment <- 
  assessment |>
  dplyr::select(conf, paper, year, 
                rev1, rev1_cp, rev1_data, rev1_methods, rev1_results, rev1_notes,
                rev2, rev2_cp, rev2_data, rev2_methods, rev2_results, rev2_notes,
                disagr_type, disagr_id) |>
  dplyr::arrange(conf, year)

  


```

Selected/relevant columns are: `{r} stringr::str_flatten_comma(names(assessment))`. More details [here](https://github.com/nuest/reproducible-research-giscience-longitudinal-study/tree/main/data-clean).


```{r}
#| label: basic-counts
#| message: false

n_papers <- nrow(assessment)
n_papers_agile <- 
  assessment |>
  count(conf) |>
  filter(conf == "agile") |>
  pull(n)


n_papers_giscience <- 
  assessment |>
  count(conf) |>
  filter(conf == "giscience") |>
  pull(n)

#n_cp <- nrow(filter(assessment, consolidated_cp == TRUE))
#p_cp <- n_cp / n_papers

```


## Types of disagreements


### Overview: all disagreement types together 

These are the types of disagreements (`disagr_type`) between the individual reviewers' scores.

- **no disagreement**: Reviewers' scores are the same in all dimensions (conceptual paper, data, methods, results)

- **borderline conceptual paper**: Disagreement are due to a different understanding whether a paper is completely conceptual; some reviewers may see a paper that only has demonstrative, synthetic example data as not “data-based” or empirical and exclude it from the assessment.

- **annotation inconsistencies**: Disagreements are due to errors in the level/score selected given the written notes of either reviewer. No matter the dimension where inconsistencies are.

- **uncertain assessment (according to protocol assessment)**: either reviewer wrote that they are unsure about level x or y, so we could just change that to the level matching the other reviewer. 

- **significant disagreement**: when reviewers' notes (or lack of notes) cannot explain differences in assessment and conversation to get an agreement is necessary and/or checking again the paper is required. **TODO: Replace `major disagreement` by `significant disagreement` in dataset**



```{r}
#| label: tbl-disagr-types
#| tbl-cap: "Distribution of disagreement types"

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("No grouping (N={n_papers})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::tab_style(
    style = cell_text(color = "#ae8b2d", weight = "bold"),
    locations = cells_body(
      columns = everything(),
      #rows = n == max(n)
      rows = n >= 50
    )
  ) |>
  gt::cols_label(
    disagr_type = md("**Disagreement type**"),
    n = md("**#**"),
    p = md("**%**")
  )

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::slice_max(order_by = p, n = 1) |>
  pull(p) -> p_disagr_top1


assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::slice_max(order_by = p, n = 2) |>
  dplyr::summarize(total = sum(p)) |>
  dplyr::pull(total) -> p_disagr_top2

```


@tbl-disagr-types shows the two most common disagreement types cover (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top2)`**). This represents almost three-quarters of all evaluations, which were easily resolved as they did not imply serious discrepancies, but rather slightly different interpretations of how the evaluation criteria were applied. However, this is important because the uncertainty in the application/interpretation of the assessment criteria covers a large proportion of the disagreements (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top1)`**), so there is room to improve and clarify aspects of the *Assessment Protocol* document to avoid ambiguities.

```{r}
#| label: tbl-disagr-types-conf
#| tbl-cap: "Distribution of disagreement types per conference: AGILE vs GIScience"

assessment |>
  dplyr::group_by(conf) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(conf, desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("Grouped by conference (N={n_papers})") 
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::summary_rows(
    groups = everything(),
    columns = n,
    fns = list(
      total = "sum"
    )
  ) |>
  gt::cols_label(
    disagr_type = "",
    n = md("**#**"),
    p = md("**%**")
  )

```


In general, @tbl-disagr-types-conf shows no remarkable differences between disagreement types per conference. Let's focus on the three intermediate disagreement types which have different percentages. AGILE papers present greater percentage of `major disagreement`, while GIScience papers have more papers assigned to `borderline conceptual paper ` and `no disagreement`. Speculating a bit, we can say that GIScience papers tend to be more theoretical than AGILE articles, which are typically classified as applied research or application papers. Following this line of argument, GIScience papers may be much easier to evaluate than AGILE papers, in which  greater uncertainty may arise when scoring the level of reproducibility for data, methods, and results. That is, more theoretical papers often use fewer computational methods and data artifacts than applied research papers, so it makes sense that `borderline conceptual paper` and `no disagreement` are common disagreement types in theory-oriented papers. Therefore, the nature of research (I mean, the number of theory-oriented papers vs the number of application-oriented papers) between the two conferences could be one of the factor that explain these small, but still significant, differences in `no disagreement`, `major disagreement`, and `borderline conceptual paper`.



```{r}
#| label: tbl-disagr-types-conf-year
#| tbl-cap: "Distribution of disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

disagr_agile <-
  assessment |>
  dplyr::filter(conf =="agile") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))


disagr_agile |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )


disagr_giscience <-
  assessment |>
  dplyr::filter(conf =="giscience") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))

disagr_giscience |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )

```


```{r}
#| label: fig-disagr-types-conf-year
#| fig-cap: "Distribution of disagreement types per conference and year (plots)"
#| fig-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2
#| column: page


disagrLinePlot <- function(data, main) {
  ggplot2::ggplot(data, 
                  aes(x = year, y = p, color = disagr_type)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point(size = 3) +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +  # Format y-axis as %
  ggplot2::guides(
      color = guide_legend(ncol = 1, position = "right")) +
  ggplot2::labs(
    title = main,
    x = "Year",
    y = "Proportion [%]",
    color = "Disagreement Type"
  ) +
  ggplot2::theme_minimal()
  
}
  

fig_agile <- disagrLinePlot(disagr_agile, "Proportion of Disagreement Types by Year (AGILE)")
fig_giscience <- disagrLinePlot(disagr_giscience, "Proportion of Disagreement Types by Year (GIScience)")

fig_agile
fig_giscience


# Taken from https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-reproducibility-assessment.Rmd
#fig_barplot <- patchwork::wrap_plots(
#  ncol = 5,
#  criteriaBarplot(`input data`,    main = "Input data",    colour = 1),
#  criteriaBarplot(`method/analysis/processing`,
#                  main = "Methods/Analysis/\nProcessing",  colour = 3),
#  criteriaBarplot(`computational environment`,
#)

#fig_barplot
```

Looking into the data over years, @tbl-disagr-types-conf-year breaks disagreement types per conference and year. @fig-disagr-types-conf-year represents the same data as line charts. 

- `borderline conceptual paper` disagreements follow similar trend in both conferences. It is almost residual in recent years (1 paper per year/conference). It was different in the first year of the series (AGILE 2017, GIScience 2016) where more disagreements of this type occurred.
- `annotation inconsistencies` is residual.
- `no disagreement`, `uncertain assessment` and `major disagreement` appear to be somewhat related. In general, when `no disagreement` increases, the other two decrease. And viceversa.
- `no disagreement` tends to decrease over time in both conferences, suggesting that agreement between reviewers in assessing the level of reproducibility in the first years of the series (AGILE 2017, GIScience 2026, GIScience 2018) was *much easier* than in recent years. Why? Possible reasons:

  - Authors have increasingly adopted best practices for describing or including other research resources used in their articles, which were often omitted in articles in earlier conferences.
  - The number of computational papers at recent conferences tends to be higher than in previous years.
  
- `uncertain assessment` and `major disagreement`. Together, it is hard to interpret them. Well, a pattern is visible over the last three conference editions: From a maximum peak (AGILE 2022, GIScience 2020), percentage of `uncertain assessment` decreases while that of  `major disagreement` increases for both conferences. Yet, the increase of `major disagrement` is greater in the last GIScience conference. Why: Perhaps the lack of explicit reproducibility guidelines for GIScience authors may have a factor as the number of computational papers in GIScience goes up.
  

### Only `no disagreement`, `uncertain assessment` and `major disagreement`



```{r}
#| label: tbl-3top-disagr-types-conf-year
#| tbl-cap: "Distribution of 3-top disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

disagr_agile |>
  dplyr::filter(disagr_type %in% c("no disagreement", "uncertain assessment", "major disagreement")) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = c("no disagreement", "uncertain assessment", "major disagreement")
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )


disagr_giscience |>
  dplyr::filter(disagr_type %in% c("no disagreement", "uncertain assessment", "major disagreement")) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = c("no disagreement", "uncertain assessment", "major disagreement")
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )
```



@tbl-3top-disagr-types-conf-year "zooms" in @@tbl-disagr-types-conf-year to show in detail the 3-top disagreement types.

- it's pretty clear that `no disagreement` has steadily declined since the beginning of the series, reaching its lowest percentage in the final year of the series for both conferences. However this common pattern develops differently for each conference: `no dissagrement` halved in AGILE 2018 and remained stable at this trend until AGILE 2024. 

- For the same year of AGILE papers, the highest percentages (in red) of `uncertain assessment` coincide with the lowest percentages (blue) of `major disagreement` (in blue). And the opposite is true too. However, this pattern, although present, is not repeated as clearly in the GIScience articles.


## Looking at individdual types of disagreements

We explore the scores (data, methods, and results) of the two reviewers for each type of disagreement, excluding `no disagrement`, `borderline conceptual paper`, and `annotation inconsistencies`.


### Type: `uncertain assessment`

```{r}
#| label: disagr-uncertain

n_disagr_uncertain <- 
  assessment |>
  filter(disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)

n_disagr_uncertain_agile <-
  assessment |>
  filter(conf == "agile", disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)

n_disagr_uncertain_giscience <-
  assessment |>
  filter(conf == "giscience", disagr_type == "uncertain assessment") |>
  count() |>
  pull(n)


disagr_uncertain <- 
  assessment |>
  dplyr::select(-paper, -rev1, -rev2, -rev1_cp, -rev2_cp, -rev1_notes, -rev2_notes) |>
  dplyr::filter(disagr_type == "uncertain assessment") |>
  dplyr::mutate(disagr_dat = (rev1_data != rev2_data)) |>
  dplyr::mutate(disagr_met = (rev1_methods != rev2_methods)) |>
  dplyr::mutate(disagr_res = (rev1_results != rev2_results)) 



disagr_uncertain |>
  dplyr::group_by(disagr_dat) |>
  dplyr::count()




```

```{r}
#| label: disagr-uncertain-conf-year


tbl_disagr_uncertain <-
  bind_cols(
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_dat) |>
    dplyr::count(name = "n_dat") |>
    dplyr::ungroup(disagr_dat) |>
    dplyr::mutate(p_dat = n_dat / sum(n_dat)) |>
    dplyr::ungroup(),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_met) |>
    dplyr::count(name = "n_met") |>
    dplyr::ungroup(disagr_met) |>
    dplyr::mutate(p_met = n_met / sum(n_met)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_res) |>
    dplyr::count(name = "n_res") |>
    dplyr::ungroup(disagr_res) |>
    dplyr::mutate(p_res = n_res / sum(n_res)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf)
  )

tbl_disagr_uncertain |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("Uncertain disagreement (N={n_disagr_uncertain})"),
    subtitle = "Applied to Data, Methods and Results scores"
  ) |>
  gt::tab_spanner(
    label = md("**Data**"),
    columns = ends_with("_dat"),
  ) |>
  gt::tab_spanner(
    label = md("**Methods**"),
    columns = ends_with("_met"),
  ) |>
  gt::tab_spanner(
    label = md("**Results**"),
    columns = ends_with("_res"),
  ) |>
  gt::fmt_percent(
    columns = c(p_dat, p_met, p_res),
    decimals = 1
  ) |>
  gt::cols_label(
    conf = md("**Conf Series**"),
    disagr_dat = md("*Distinct scores?*"),
    disagr_met = md("*Distinct scores?*"),
    disagr_res = md("*Distinct scores?*"))

```




### Type: `major disagreement`





