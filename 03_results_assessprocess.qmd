---
title: "03_results_assessprocess"
description: |
    Supporting notebook to analyse the results of assessment process to uncover the reasons behind disagreements found among assessors, so that the AGILE Reproducibility Guidelines and the Assessemnt Protocol can be updated accordingly. See the **Results** section in the final article. 

#execute:
#  echo: false
format: 
  html:
    toc: true
    embed-resources: true  # Self-contained
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
---

## Introduction

The majority of the work happened during the assessment process: evaluation of the papers, consolidation of the assessments, and **synthesis of the rich feedback** as well as interpretation of the findings. Here, we retrieve data items required to explore feedback from assessors during the assessment process. Individual scores of conceptual paper, data, methods, and results (computational environment is skipped, as it is not relevant for discrepancies analysis), notes from the two assessors (A1 and A2), and **type of disagreement** are relevant here.

```{r}
#| label: load-packages
#| message: false

library(here)
library(tidyverse)
library(dplyr)
library(scales)
library(stringr)
library(gt)
library(ggplot2)
```


```{r}
#| label: load-data
#| message: false

assessment <- 
  readr::read_csv(here::here("data-clean", "all-data.csv"))

assessment <- 
  assessment |>
  dplyr::select(conf, paper, year, 
                rev1, rev1_cp, rev1_data, rev1_methods, rev1_results, rev1_notes,
                rev2, rev2_cp, rev2_data, rev2_methods, rev2_results, rev2_notes,
                disagr_type, disagr_id) |>
  dplyr::arrange(conf, year)


```

Selected/relevant columns are: `{r} stringr::str_flatten_comma(names(assessment))`. More details [here](https://github.com/nuest/reproducible-research-giscience-longitudinal-study/tree/main/data-clean).

```{r}
#| label: basic-counts
#| message: false

n_papers <- nrow(assessment)
n_papers_agile <- 
  assessment |>
  count(conf) |>
  filter(conf == "agile") |>
  pull(n)


n_papers_giscience <- 
  assessment |>
  count(conf) |>
  filter(conf == "giscience") |>
  pull(n)


```

## Types of disagreements

### Overview: all disagreement types together

These are the types of disagreements (`disagr_type`) between the individual reviewers' scores.

-   **no disagreement**: Assessors' scores are the same in all dimensions (conceptual paper, data, methods, results)

-   **borderline conceptual paper**: Disagreement are due to a different understanding whether a paper is completely conceptual; some assessors may see a paper that only has demonstrative, synthetic example data as not “data-based” or empirical and exclude it from the assessment.

-   **annotation inconsistencies**: Disagreements are due to errors in the level/score selected given the written notes of either assessor. No matter the dimension where inconsistencies are.

-   **uncertain assessment (according to protocol assessment)**: either assessor wrote that they are unsure about level x or y, so we could just change that to the level matching the other assessor

-   **significant disagreement**: when assessors' notes (or lack of notes) cannot explain differences in assessment and conversation to get an agreement is necessary and/or checking again the paper is required.


@tbl-disagr-types below corresponds to the **Table 3: Distribution of disagreement types** of the published paper. 

```{r}
#| label: tbl-disagr-types
#| tbl-cap: "Distribution of disagreement types"

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("No grouping (N={n_papers}). Table 3 of the published paper")
  ) |>
  gt::grand_summary_rows(
    columns = c("n","p"),
    fns = list(
      label = md("**Total**"),
      fn = "sum")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = md("**Disagreement type**"),
    n = md("**#**"),
    p = md("**%**")
  )


```

```{r}
#| label: globals-disagr-types-conf
#| message: false

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::slice_max(order_by = p, n = 1) |>
  pull(p) -> p_disagr_top1

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::filter(disagr_type != "no disagreement") |> # Exclude agreement, of course
  dplyr::slice_max(order_by = p, n = 2) |>
  dplyr::summarize(sum = sum(p)) |>
  dplyr::pull(sum) -> p_disagr_top2

assessment |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::filter(disagr_type == "no disagreement") |> # Only total agreement
  dplyr::pull(p) -> p_no_disagr
```

First, the assessors made the same assessment in **`{r} scales::label_percent(accuracy = 0.1)(p_no_disagr)`** of the papers. Assessors disagreed in more than 70% of the total number of papers (N=`{r} n_papers`)!

Second, @tbl-disagr-types also shows that, after excluding `no disagreement` (well, it means perfect match between assessors!), the two most common disagreement types cover (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top2)`**). Uncertainty in the application/interpretation of the assessment criteria covers a large proportion of the disagreements (**`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top1)`**), so there is room to improve and clarify aspects of the *Assessment Protocol* document to avoid slightly different interpretations of how the evaluation criteria have to be applied.



```{r}
#| label: tbl-disagr-types-conf
#| tbl-cap: "Distribution of disagreement types per conference: AGILE vs GIScience"

assessment |>
  dplyr::group_by(conf) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(conf, desc(n)) |>
  gt::gt() |>
  gt::tab_header(
    title = "Distribution of disagreement types",
    subtitle = stringr::str_glue("Grouped by conference (N={n_papers})") 
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::summary_rows(
    groups = everything(),
    columns = n,
    fns = list(
      total = "sum"
    )
  ) |>
  gt::cols_label(
    disagr_type = "",
    n = md("**#**"),
    p = md("**%**")
  )

```

In general, @tbl-disagr-types-conf shows no remarkable differences between disagreement types per conference. Let's focus on the three intermediate disagreement types which have different percentages. AGILE papers present greater percentage of `significant disagreement`, while GIScience papers have more papers assigned to `borderline conceptual paper` and `no disagreement`. Speculating a bit, we can say that GIScience papers tend to be more theoretical than AGILE articles, which are typically classified as applied research or application papers. Following this line of argument, GIScience papers may be much easier to evaluate than AGILE papers, in which greater uncertainty may arise when scoring the level of reproducibility for *Data*, *Methods*, and *Results*. That is, more theoretical papers often use fewer computational methods and data artifacts than applied research papers, so it makes sense that `borderline conceptual paper` and `no disagreement` are common disagreement types in theory-oriented papers. Therefore, the nature of research (I mean, the number of theory-oriented papers vs the number of application-oriented papers) between the two conferences could be one of the factor that explain these small, but still significant, differences in `no disagreement`, `significant disagreement`, and `borderline conceptual paper`.

```{r}
#| label: tbl-disagr-types-conf-year
#| tbl-cap: "Distribution of disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

disagr_agile <-
  assessment |>
  dplyr::filter(conf =="agile") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))


disagr_agile |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )


disagr_giscience <-
  assessment |>
  dplyr::filter(conf =="giscience") |>
  dplyr::group_by(year) |>
  dplyr::count(disagr_type) |>
  dplyr::mutate(p = n /sum(n)) |>
  dplyr::arrange(year, desc(n))

disagr_giscience |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::fmt_percent(
    columns = p,
    decimals = 1
  ) |>
  gt::cols_label(
    disagr_type = "",
    year = md("Year"),
    n = md("**#**"),
    p = md("**%**")
  )

```

```{r}
#| label: fig-disagr-types-conf-year
#| fig-cap: "Distribution of disagreement types per conference and year (plots)"
#| fig-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2
#| fig-column: page


disagrLinePlot <- function(data, main) {
  ggplot2::ggplot(data, 
                  aes(x = year, y = p, color = disagr_type)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point(size = 3) +
  ggplot2::scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +  # Format y-axis as %
  ggplot2::guides(
      color = guide_legend(ncol = 1, position = "right")) +
  ggplot2::labs(
    title = main,
    x = "Year",
    y = "Proportion [%]",
    color = "Disagreement Type"
  ) +
  ggplot2::theme_minimal()
  
}
  

fig_agile <- disagrLinePlot(disagr_agile, "Proportion of Disagreement Types by Year (AGILE)")
fig_giscience <- disagrLinePlot(disagr_giscience, "Proportion of Disagreement Types by Year (GIScience)")

fig_agile
fig_giscience


```

Looking into the data over years, @tbl-disagr-types-conf-year breaks disagreement types per conference and year. @fig-disagr-types-conf-year represents the same data as line charts.

-   `borderline conceptual paper` disagreements follow similar trend in both conferences. It is almost residual in recent years (1 paper per year/conference). It was different in the first year of the series (AGILE 2017, GIScience 2016) where more disagreements of this type occurred.

-   `annotation inconsistencies` is residual.

-   `no disagreement`, `uncertain assessment` and `significant disagreement` appear to be somewhat related. In general, when `no disagreement` increases, the other two decrease. And viceversa.

-   `no disagreement` tends to decrease over time in both conferences, suggesting that agreement between assessors in assessing the level of reproducibility in the first years of the series (AGILE 2017, GIScience 2026, GIScience 2018) was *much easier* than in recent years. Why? Possible reasons:

    -   Authors have increasingly adopted best practices for describing or including other research resources used in their articles, which were often omitted in articles in earlier conferences.
    -   The number of computational papers at recent conferences tends to be higher than in previous years.

-   `uncertain assessment` and `significant disagreement`. Together, it is hard to interpret them. Well, a pattern is visible over the last three conference editions: From a maximum peak (AGILE 2022, GIScience 2020), percentage of `uncertain assessment` decreases while that of `significant disagreement` increases for both conferences. Yet, the increase of `significant disagrement` is greater in the last GIScience conference. Why: Perhaps the lack of explicit reproducibility guidelines for GIScience authors may have a factor as the number of computational papers in GIScience goes up.

### Top3 types of discrepancies: `no disagreement`, `uncertain assessment` and `significant disagreement`

```{r}
#| label: tbl-top3-disagr-types-conf-year
#| tbl-cap: "Distribution of 3-top disagreement types per conference and year"
#| tbl-subcap: 
#|     - "AGILE"
#|     - "GIScience"
#| layout-ncol: 2

top3 = c("no disagreement", "uncertain assessment", "significant disagreement")

disagr_agile |>
  dplyr::filter(disagr_type %in% top3) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("AGILE conference only (N={n_papers_agile})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = top3
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )


disagr_giscience |>
  dplyr::filter(disagr_type %in% top3) |>
  dplyr::select(-n) |>
  tidyr::pivot_wider(
    names_from = "disagr_type",
    values_from = "p") |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("GIScience conference only (N={n_papers_giscience})")
  ) |>
  gt::tab_spanner(
    label = "3-top disagreement types", 
    columns = top3
  ) |>
  gt::fmt_percent(
    columns = c(everything(), -year),
    decimals = 1
  ) |>
  gt::data_color(
    columns = c(everything(), -year),
    method = "bin",
    palette = "RdBu",
    reverse = TRUE
  )
```

@tbl-top3-disagr-types-conf-year corresponds to the **Figure 2: Distribution of 3-top disagreement types per conference and year** of the published paper. And @tbl-top3-disagr-types-conf-year "zooms in" @tbl-disagr-types-conf-year to show in detail the 3-top types of disagreement. 

-   it's pretty clear that `no disagreement` has steadily declined since the beginning of the series, reaching its lowest percentage in the final year of the series for both conferences. However this common pattern develops differently for each conference: `no dissagrement` halved in AGILE 2018 and remained stable at this trend until AGILE 2024.

-   For the same year of AGILE papers, the highest percentages (in red) of `uncertain assessment` coincide with the lowest percentages (blue) of `major disagreement` (in blue). And the opposite is true too. However, this pattern, although present, is not repeated as clearly in the GIScience articles.

## Looking at individdual types of disagreements

We explore the scores (data, methods, and results) of the two reviewers for each type of disagreement, excluding `no disagrement`, `borderline conceptual paper`, and `annotation inconsistencies`.

### Type: `uncertain assessment`

```{r}
#| label: disagr-uncertain

curr_type = "uncertain assessment"

n_disagr_uncertain <- 
  assessment |>
  filter(disagr_type == curr_type) |>
  count() |>
  pull(n)

n_disagr_uncertain_agile <-
  assessment |>
  filter(conf == "agile", disagr_type == curr_type) |>
  count() |>
  pull(n)

n_disagr_uncertain_giscience <-
  assessment |>
  filter(conf == "giscience", disagr_type == curr_type) |>
  count() |>
  pull(n)


disagr_uncertain <- 
  assessment |>
  dplyr::select(-paper, -rev1, -rev2, -rev1_cp, -rev2_cp, -rev1_notes, -rev2_notes) |>
  dplyr::filter(disagr_type == curr_type) |>
  dplyr::mutate(disagr_dat = (rev1_data != rev2_data)) |>
  dplyr::mutate(disagr_met = (rev1_methods != rev2_methods)) |>
  dplyr::mutate(disagr_res = case_when (
    is.na(rev1_results) ~ TRUE,
    is.na(rev2_results) ~ TRUE,
    .default = (rev1_results != rev2_results)))
                
```

```{r}
#| label: tbl-disagr-uncertain-conf
#| tbl-cap: "Distribution of 'Uncertain disagreement'"

data_tbl_disagr_uncertain <-
  bind_cols(
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_dat) |>
    dplyr::count(name = "n_dat") |>
    dplyr::ungroup(disagr_dat) |>
    dplyr::mutate(p_dat = n_dat / sum(n_dat)) |>
    dplyr::ungroup(),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_met) |>
    dplyr::count(name = "n_met") |>
    dplyr::ungroup(disagr_met) |>
    dplyr::mutate(p_met = n_met / sum(n_met)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf),
    
    disagr_uncertain |>
    dplyr::group_by(conf, disagr_res) |>
    dplyr::count(name = "n_res") |>
    dplyr::ungroup(disagr_res) |>
    dplyr::mutate(p_res = n_res / sum(n_res)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf)
  )


tbl_disagr_uncertain <-
  data_tbl_disagr_uncertain |>
  dplyr::mutate(conf = stringr::str_to_upper(conf)) |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("Uncertain disagreement (N={n_disagr_uncertain})"),
    subtitle = md("Applied to *Input data*, *Methods, ...* and *Results* dimensions.")
  ) |>
  gt::tab_spanner(
    label = md("**Input Data**"),
    columns = ends_with("_dat"),
  ) |>
  gt::tab_spanner(
    label = md("**Methods, ...**"),
    columns = ends_with("_met"),
  ) |>
  gt::tab_spanner(
    label = md("**Results**"),
    columns = ends_with("_res"),
  ) |>
  gt::cols_align(
    align = c("center"),
    columns = everything()
  ) |>
  # Commented code for make table cell in bold. 
  #gt::tab_style(
  #  style = cell_text(weight = "bold"),
  #  locations = 
  #    cells_body(
  #      rows = disagr_dat == TRUE)
  #) |>
  # Commented code for coloring table cells. Replaced by footnotes (best for exporting to latex)
  #gt::tab_style(
  #  style = cell_fill(color = "lightcyan"),
  #  locations = list(
  #    cells_body(
  #      columns = p_met,
  #      rows = p_met <= 0.3),
  #    cells_body(
  #      columns = p_res,
  #      rows = p_res <= 0.2)
  #  )
  #) |>
  #gt::tab_style(
  #  style = list(
  #    cell_fill(color = "indianred"),
  #    cell_text(color = "white")
  #    ),
  #  locations = cells_body(
  #      columns = p_dat,
  #      rows = p_dat >= 0.6
  #  )
  #) |>
  gt::fmt_percent(
    columns = c(p_dat, p_met, p_res),
    decimals = 1
  ) |>
  gt::fmt_tf(
    columns = c(disagr_dat, disagr_met, disagr_res),
    tf_style = "yes-no"
  ) |>
  gt::cols_label(
    conf = md("*Conf Series*"),
    disagr_dat = md("*Disagr?*"),
    disagr_met = md("*Disagr?*"),
    disagr_res = md("*Disagr?*"),
    n_dat = md("*#*"),
    p_dat = md("*%*"),
    n_met = md("*#*"),
    p_met = md("*%*"),
    n_res = md("*#*"),
    p_res = md("*%*"),
  ) |>
  gt::tab_style(
    style = cell_borders(sides = c("bottom"),  weight = px(1.5)),
    locations = cells_body(rows = 2)
  ) |>
  gt::opt_align_table_header(align = "center") |>
  gt::tab_footnote(
    footnote = "Major disagreements at both conferences.",
    locations = cells_body(columns = p_dat, rows = c(2, 4))
  ) |>
  gt::tab_footnote(
    footnote = md("Minor disagreements at the AGILE conference."),
    locations = cells_body(columns = p_met, rows = 2)
  ) |>
  gt::tab_footnote(
    footnote = md("Minor disagreements at the GIScience conference."),
    locations = cells_body(columns = p_res, rows = 4)
  )
  #gt::tab_options(footnotes.multiline = FALSE)
  #gt::opt_all_caps()


tbl_disagr_uncertain

```


@tbl-disagr-uncertain-conf corresponds to the **Table 4: Distribution of "uncertain assessment" (N=100) per conference and criterion** of the published paper. 

What we observe in @tbl-disagr-uncertain-conf (only rows with `disagr?` == `yes`) is: 

- AGILE: 
  - Lowest proportion of `uncertain assessment` disagreements is in **Methods**, under 30%. 
  - Highest proportion of `uncertain assessment` disagreements is in **Input data**, close to 65%.
  - From higher to lower proportion of disagreements: Data -> Results -> Methods
  
- GIScience:
  - Lowest proportion of `uncertain assessment` disagreements is in **Results**, just 20%. 
  - Highest proportion of `uncertain assessment` disagreements is in **Input data**, just 80%.
  - From higher to lower proportion of disagreements: Data -> Methods -> Results 


```{r}
#| label: table-disagr-uncertain-to-latex
#| eval: false
#| echo: false

tbl_latex <- 
  tbl_disagr_uncertain |>
  gt::as_latex()

tbl_latex |>
  as.character() |>
  cat()


```




In @fig-disagr-uncertain-heatmap, we look now at the individual scores (UDAI levels) for each of the dimensions (data, methods, results), considering AGILE (top row) and GIScience (bottom row).

```{r}
#| label: fig-disagr-uncertain-heatmap
#| fig-cap: "Counts UDAI score for 'uncertain disagreement' between reviewers (heatmap): AGILE vs GIScience"
#| fig-subcap: 
#|     - "AGILE: Data"
#|     - "AGILE: Methods"
#|     - "AGILE: Results"
#|     - "GIScience: Data"
#|     - "GIScience: Methods"
#|     - "GIScience: Results"
#| layout: [[30, 30, 30], [30, 30, 30]]
#| fig-column: page

# TODO: https://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization

udao_levels <-  c("U", "D", "A", "O")

heatmap_agile_dat <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_data, rev2_data) |>
    dplyr::mutate(rev1_data = factor(rev1_data, levels = udao_levels)) |>
    dplyr::mutate(rev2_data = factor(rev2_data, levels = udao_levels)) |>
    dplyr::group_by(rev1_data,rev2_data) %>%
    dplyr::tally()


heatmap_agile_met <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_methods, rev2_methods) |>
    dplyr::mutate(rev1_methods = factor(rev1_methods, levels = udao_levels)) |>
    dplyr::mutate(rev2_methods  = factor(rev2_methods, levels = udao_levels)) |>
    dplyr::group_by(rev1_methods,rev2_methods) %>%
    dplyr::tally()


heatmap_agile_res <-
  disagr_uncertain |>
    dplyr::filter(conf=="agile") |>
    dplyr::select(year, rev1_results, rev2_results) |>
    dplyr::mutate(rev1_results = factor(rev1_results, levels = udao_levels)) |>
    dplyr::mutate(rev2_results = factor(rev2_results, levels = udao_levels)) |>
    dplyr::group_by(rev1_results,rev2_results) %>%
    dplyr::tally()


heatmap_giscience_dat <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_data, rev2_data) |>
    dplyr::mutate(rev1_data = factor(rev1_data, levels = udao_levels)) |>
    dplyr::mutate(rev2_data = factor(rev2_data, levels = udao_levels)) |>
    dplyr::group_by(rev1_data,rev2_data) %>%
    dplyr::tally()


heatmap_giscience_met <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_methods, rev2_methods) |>
    dplyr::mutate(rev1_methods = factor(rev1_methods, levels = udao_levels)) |>
    dplyr::mutate(rev2_methods  = factor(rev2_methods, levels = udao_levels)) |>
    dplyr::group_by(rev1_methods,rev2_methods) %>%
    dplyr::tally()


heatmap_giscience_res <-
  disagr_uncertain |>
    dplyr::filter(conf=="giscience") |>
    dplyr::select(year, rev1_results, rev2_results) |>
    dplyr::mutate(rev1_results = factor(rev1_results, levels = udao_levels)) |>
    dplyr::mutate(rev2_results = factor(rev2_results, levels = udao_levels)) |>
    dplyr::group_by(rev1_results,rev2_results) %>%
    dplyr::tally()


heatmap_agile_dat |>
  ggplot2::ggplot(aes(x = rev1_data, y = rev2_data, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Data disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

heatmap_agile_met |>
  ggplot2::ggplot(aes(x = rev1_methods, y = rev2_methods, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Methods disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_agile_res |>
  ggplot2::ggplot(aes(x = rev1_results, y = rev2_results, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "AGILE: Results disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_giscience_dat |>
  ggplot2::ggplot(aes(x = rev1_data, y = rev2_data, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Data disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

heatmap_giscience_met |>
  ggplot2::ggplot(aes(x = rev1_methods, y = rev2_methods, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Methods disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()


heatmap_giscience_res |>
  ggplot2::ggplot(aes(x = rev1_results, y = rev2_results, fill = n)) +
  ggplot2::geom_tile(color = "white") +
  ggplot2::geom_text(aes(label = n), color = "white", size = 4) +
  ggplot2::scale_fill_gradient(low = "blue", high = "red") + 
  ggplot2::coord_fixed() +
  ggplot2::labs(title = "GIScience: Results disagreement",
       x = "Reviewer 1",
       y = "Reviewer 2") +
  ggplot2::theme_minimal()

```

  

### Type: `significant disagreement`


```{r}
#| label: disagr-significant

curr_type = "significant disagreement"

n_disagr_significant <- 
  assessment |>
  dplyr::filter(disagr_type == curr_type) |>
  dplyr::count() |>
  dplyr::pull(n)

n_disagr_significant_agile <-
  assessment |>
  dplyr::filter(conf == "agile", disagr_type == curr_type) |>
  dplyr::count() |>
  dplyr::pull(n)

n_disagr_significant_giscience <-
  assessment |>
  dplyr::filter(conf == "giscience", disagr_type == curr_type) |>
  dplyr::count() |>
  dplyr::pull(n)


disagr_significant <- 
  assessment |>
  dplyr::select(-paper, -rev1, -rev2, -rev1_cp, -rev2_cp, -rev1_notes, -rev2_notes) |>
  dplyr::filter(disagr_type == curr_type) |>
  dplyr::mutate(disagr_dat = (rev1_data != rev2_data)) |>
  dplyr::mutate(disagr_met = (rev1_methods != rev2_methods)) |>
  dplyr::mutate(disagr_res = (rev1_results != rev2_results)) 


```


```{r}
#| label: tbl-disagr-significant-conf
#| tbl-cap: "Distribution of 'significant disagreement'"

tbl_disagr_significant <-
  bind_cols(
    disagr_significant |>
    dplyr::group_by(conf, disagr_dat) |>
    dplyr::count(name = "n_dat") |>
    dplyr::ungroup(disagr_dat) |>
    dplyr::mutate(p_dat = n_dat / sum(n_dat)) |>
    dplyr::ungroup(),
    
    disagr_significant |>
    dplyr::group_by(conf, disagr_met) |>
    dplyr::count(name = "n_met") |>
    dplyr::ungroup(disagr_met) |>
    dplyr::mutate(p_met = n_met / sum(n_met)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf),
    
    disagr_significant |>
    dplyr::group_by(conf, disagr_res) |>
    dplyr::count(name = "n_res") |>
    dplyr::ungroup(disagr_res) |>
    dplyr::mutate(p_res = n_res / sum(n_res)) |>
    dplyr::ungroup() |>
    dplyr::select(-conf)
  )

tbl_disagr_significant |>
  gt::gt() |>
  gt::tab_header(
    title = stringr::str_glue("Significant disagreement (N={n_disagr_significant})"),
    subtitle = md("Applied to *Data*, *Methods* and *Results* scores (of the two reviewers).<br>The lowest disagreements are in cyan, the highest disagreements in red.")
  ) |>
  gt::tab_spanner(
    label = md("**Data**"),
    columns = ends_with("_dat"),
  ) |>
  gt::tab_spanner(
    label = md("**Methods**"),
    columns = ends_with("_met"),
  ) |>
  gt::tab_spanner(
    label = md("**Results**"),
    columns = ends_with("_res"),
  ) |>
  gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = 
      cells_body(
        rows = disagr_dat == TRUE)
  ) |>
  gt::tab_style(
    style = list(
      cell_fill(color = "indianred"),
      cell_text(color = "white")
      ),
    locations = cells_body(
        columns = p_dat,
        rows = p_dat >= 0.7
    )
  ) |>
  gt::fmt_percent(
    columns = c(p_dat, p_met, p_res),
    decimals = 1
  ) |>
  gt::cols_label(
    conf = md("**Conf Series**"),
    disagr_dat = md("*Disagreement?*"),
    disagr_met = md("*Disagreement?*"),
    disagr_res = md("*Disagreement?*"))

```

What we observe in @tbl-disagr-significant-conf (rows in **bold** only): 

- AGILE: 
  - Lowest proportion of `major disagreement` is in **Results**. 
  - Highest proportion of `major disagreement` is in **Data**. 
  - From higher to lower proportion of disagreements: Data -> Methods -> Results
  
- GIScience:
  - Lowest proportion of `major disagreement` is in **Methods and Results**, same proportion. 
  - Highest proportion of `uncertain disagreement` is in **Data**.
  - From higher to lower proportion of disagreements: Data -> Methods/Results 


## Remarks
 
The exploratory analysis above discerns some observations (unsure if we can term them as *patterns*) about the disagreements found between assessors while evaluating the level of reproducibility of AGILE/GIScience papers. The following observations should be considered simply as factual information (which can be converted into recommendations) to improve the clarity of the AGILE Guidelines and the Assessment Protocol.

- Reviewers disagreed in more than 70% of the total number of papers (N=`{r} n_papers`)! See @tbl-disagr-types

- On the positive side, **`{r} scales::label_percent(accuracy = 0.1)(p_disagr_top2)`** of disagreements where due to slightly different interpretations of the evaluation criteria, suggesting that improving the *Assessment Protocol* could drastically reduce it. On the contrary, `significant disagreement` (almost 20%) denotes that assessors had not enough information on the assessed paper to obtain a similar score. That is, determining the root causes of disagreement is difficult and, in most cases, is due to several factors. The most relevant could be a lack of detail in the original paper, along with some imprecise instructions in the **Assessment Protocol**.
 
- Similar proportions of disagreement types per conference - see @tbl-disagr-types-conf.

- See comments above regarding @tbl-disagr-types-conf-year and @fig-disagr-types-conf-year, where disagreement types are divided by conference and year. 
 
- See comments above regarding @tbl-top3-disagr-types-conf-year. 

- Regardless of the conference, the *Data* dimension accounts for the largest proportion of `uncertain assessment` disagreements when analyzing individual scores (see @tbl-disagr-uncertain-conf). That is, the assessors disagreed primarily on the data dimension. Therefore, although the disagreements discussed (`uncertain assessment`) stem from slightly subjective interpretations of the assessment protocol, the *Data* dimension remains the controversial one. **Therefore**, efforts to improve guidelines on how to clearly evaluate (by reviewers/readers) and unambiguously describe (by authors) input data are crucial.
 
- Regardless of the conference, the *Data* dimension accounts for the largest proportion of `significant disagreement` when analysing individual scores (see @tbl-disagr-significant-conf). 


