{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54970ed-da34-4cfa-87f6-3f299394b430",
   "metadata": {},
   "source": [
    "# Combined hypothesis testing\n",
    "\n",
    "This notebook combines and integrates the entire analysis workflow. It tests whether\n",
    "- there was an increase in potential reproducibility in Agile papers after the introduction of the guidelines in 2020 (A)\n",
    "- there was an increase in potential reproducibilit in GIScience papers after that introduction (B)\n",
    "- there was a difference in increase between the two conference series (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48223cd-fa0c-4154-ab0f-017f2d60f20c",
   "metadata": {},
   "source": [
    "## Preliminaries: preparing data and analysis functions\n",
    "\n",
    "This sections loads and prepares the input data, and provides the functions necessary for the hypothesis testing. It only needs to be run once. All later sections depend on this one, but are independent from one another. That means you can explore different variable settings for the hypotheses without interfering with each other. \n",
    "\n",
    "### Setup and prepare data\n",
    "You can set global variables after the necessary libraries are loaded, then the data is loaded and preprocessed. \n",
    "\n",
    "The script assumes that the available data is a CSV file with the following relevant columns and variable values (for detailed explanations of the UDAO scheme, see the corresponding paper). \n",
    "\n",
    "- conf: \"agile\" or \"giscience\"\n",
    "- year: year of the conference proceedings\n",
    "- consolidated_cp: conceptual paper true/false\n",
    "- consolidated_data: data dimension in UDAO (undocumented, documented, available, open) scheme\n",
    "- consolidated_methods: methods dimension in UDAO scheme\n",
    "- consolidated_results: results dimension in UDAO scheme\n",
    "- consolidated_ce: computational environment true/false\n",
    "\n",
    "The preprocessing requires the same two steps for all hypotheses:\n",
    "\n",
    "1. We need to remove all conceptual papers.\n",
    "2. For basic calculations on ranks (see discussion on that in a later section), we need to convert the UDAO scheme into ranks 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7011b4ff-5749-49a1-bf3f-472cdb50b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# change variables as needed\n",
    "INPUT_CSV = '../data-clean/all-data.csv'\n",
    "CONF_COL = 'conf'\n",
    "YEAR_COL = 'year'\n",
    "TEST_COL = ['consolidated_data', 'consolidated_methods', 'consolidated_results']\n",
    "REPLACE = {\n",
    "    'Not applicable': np.nan,\n",
    "    'U': 0,\n",
    "    'D': 1,\n",
    "    'A': 2,\n",
    "    'O': 3\n",
    "}\n",
    "P_VALUE=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d95517-13a6-475a-8fe3-2f1c0774cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 203\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# remove conceptual papers\n",
    "mask = df['consolidated_cp'] == True\n",
    "df_nocp = df[~mask]\n",
    "\n",
    "print(len(df), len(df_nocp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06af401-5bd8-42b9-bda5-b530d6b79afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OstermannFO\\AppData\\Local\\Temp\\ipykernel_17336\\324311168.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nocp[column] = df_nocp[column].replace(REPLACE).astype('Int64')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conf</th>\n",
       "      <th>paper</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>oa</th>\n",
       "      <th>dasa</th>\n",
       "      <th>rev1</th>\n",
       "      <th>rev2</th>\n",
       "      <th>rev1_cp</th>\n",
       "      <th>...</th>\n",
       "      <th>consolidated_cp</th>\n",
       "      <th>consolidated_data</th>\n",
       "      <th>consolidated_methods</th>\n",
       "      <th>consolidated_results</th>\n",
       "      <th>consolidated_ce</th>\n",
       "      <th>consolidated_notes</th>\n",
       "      <th>disagr_type</th>\n",
       "      <th>agile_badge</th>\n",
       "      <th>agile_reproreport</th>\n",
       "      <th>disagr_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agile</td>\n",
       "      <td>agile_2017_006</td>\n",
       "      <td>2017</td>\n",
       "      <td>Follow the Signs—Countering Disengagement from...</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>FO</td>\n",
       "      <td>AG</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>no disagreement</td>\n",
       "      <td>no disagreement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agile</td>\n",
       "      <td>agile_2017_014</td>\n",
       "      <td>2017</td>\n",
       "      <td>The Effect of Regional Variation and Resolutio...</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>FO</td>\n",
       "      <td>AG</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>no disagreement</td>\n",
       "      <td>no disagreement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agile</td>\n",
       "      <td>agile_2019_003</td>\n",
       "      <td>2019</td>\n",
       "      <td>Evaluating the Effectiveness of Embeddings in ...</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>FO</td>\n",
       "      <td>AG</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>rev1 correct; broken link to Twitter dataset; ...</td>\n",
       "      <td>uncertain assessment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    conf           paper  year  \\\n",
       "0  agile  agile_2017_006  2017   \n",
       "1  agile  agile_2017_014  2017   \n",
       "2  agile  agile_2019_003  2019   \n",
       "\n",
       "                                               title  \\\n",
       "0  Follow the Signs—Countering Disengagement from...   \n",
       "1  The Effect of Regional Variation and Resolutio...   \n",
       "2  Evaluating the Effectiveness of Embeddings in ...   \n",
       "\n",
       "                                                link  oa dasa rev1 rev2  \\\n",
       "0  https://link.springer.com/chapter/10.1007/978-...  no   no   FO   AG   \n",
       "1  https://link.springer.com/chapter/10.1007/978-...  no   no   FO   AG   \n",
       "2  https://link.springer.com/chapter/10.1007/978-...  no   no   FO   AG   \n",
       "\n",
       "   rev1_cp  ...  consolidated_cp consolidated_data consolidated_methods  \\\n",
       "0    False  ...            False                 1                    1   \n",
       "1    False  ...            False                 1                    1   \n",
       "2    False  ...            False                 2                    1   \n",
       "\n",
       "  consolidated_results consolidated_ce  \\\n",
       "0                    1           False   \n",
       "1                    1           False   \n",
       "2                    1           False   \n",
       "\n",
       "                                  consolidated_notes           disagr_type  \\\n",
       "0                                    no disagreement       no disagreement   \n",
       "1                                    no disagreement       no disagreement   \n",
       "2  rev1 correct; broken link to Twitter dataset; ...  uncertain assessment   \n",
       "\n",
       "   agile_badge  agile_reproreport disagr_id  \n",
       "0          NaN                NaN         0  \n",
       "1          NaN                NaN         0  \n",
       "2          NaN                NaN         3  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert values\n",
    "for column in TEST_COL:\n",
    "    df_nocp[column] = df_nocp[column].replace(REPLACE).astype('Int64')\n",
    "\n",
    "df_nocp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a4fcf-e12f-4553-983d-a1e54280fbf6",
   "metadata": {},
   "source": [
    "### Setup analysis functions\n",
    "\n",
    "For testing hypotheses A and B, we have\n",
    "\n",
    "- two independent groups (pre- and post-intervention) with unequal sample sizes\n",
    "- the data is ranked (i.e., a value of 4 is better than a value of 2, but not double)\n",
    "\n",
    "The question is whether any one group has significantly higher or lower ranks than the other.\n",
    "\n",
    "The suitable test is Mann-Whitney U (Wilcoxon rank-sum test) which\n",
    "\n",
    "- is non-parametric\n",
    "- compares sum of ranks between groups\n",
    "- suited for ordinal data\n",
    "- robust to different sample sizes\n",
    "\n",
    "The analysis function takes two groups and a column to test on and returns a dataframe. A secondary analysis function for descriptive statistics takes the same input and returns also a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b2ab237-6a86-4ffb-8c06-7f5ad0742397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_whitney_u(group_1, group_2, column):\n",
    "    # Perform the Mann–Whitney U test\n",
    "    stat, p_value = mannwhitneyu(group_1[column], group_2[column], alternative='two-sided')\n",
    "\n",
    "    # Add the results to data frame\n",
    "    u_stats= pd.DataFrame([\n",
    "        {\n",
    "            \"Criterion\": column, \n",
    "            \"U-Statistic\": stat, \n",
    "            \"P-value\": p_value\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    return u_stats\n",
    "\n",
    "\n",
    "def descr_stats(group_1, group_2, column):\n",
    "    # Collect results in a data frame\n",
    "    d_stats = pd.DataFrame([\n",
    "        {\n",
    "            \"Group\": \"Pre-Intervention\",\n",
    "            \"criterion\": column,\n",
    "            \"mean\": group_1[column].mean(),\n",
    "            \"mode\": group_1[column].mode().iloc[0],\n",
    "            \"median\": int(group_1[column].median())\n",
    "        },\n",
    "        {\n",
    "            \"Group\": \"Post-Intervention\",\n",
    "            \"criterion\": column,\n",
    "            \"mean\": group_2[column].mean(),\n",
    "            \"mode\": group_2[column].mode().iloc[0],\n",
    "            \"median\": int(group_2[column].median())\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    return d_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0167b11-08b1-4fb8-bdeb-b5b8f689095a",
   "metadata": {},
   "source": [
    "For testing Hypothesis C, we have\n",
    "\n",
    "- two independent groups (conferences)\n",
    "- for each group, we have two independent sub-groups (pre- and post-intervention) with unequal sample sizes\n",
    "- the data is ranked (i.e., a value of 4 is better than a value of 2, but not double)\n",
    "\n",
    "The question is whether any difference in ranking before and after is different between the two conferences.\n",
    "\n",
    "An often suggested approach to compare the change over time between two groups with ordinal data is Aligned Rank Transform Analysis of Variance (e.g., ARTool package in R). However, there have been raised serious concerns over its reliability (https://statransform.github.io/jovi/). \n",
    "\n",
    "An alternative would be to run another MannWhitneyU on the deltas between the two conference groups, but that would require quantitatively meaningful differences between ranks (e.g., a rank of 4 is twice that of 2), which we don't have (see above). \n",
    "\n",
    "That leaves a descriptive comparison between effect sizes, e.g., by computing the rank-biserial for both conference groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5691f18d-45b9-456c-896d-67da96cfb059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_biserial(group_1, group_2, column):\n",
    "    # Perform the Mann–Whitney U test\n",
    "    stat, p_value = mannwhitneyu(group_1[column], group_2[column], alternative='two-sided')\n",
    "\n",
    "    # Sample sizes\n",
    "    n1 = len(group_1)\n",
    "    n2 = len(group_2)\n",
    "\n",
    "    # Compute rank-biserial correlation\n",
    "    r = 1 - (2 * stat) / (n1 * n2)\n",
    "\n",
    "    # Add the results to data frame\n",
    "    effect_stats= pd.DataFrame([\n",
    "        {\n",
    "            \"Criterion\": column, \n",
    "            \"U-Statistic\": stat, \n",
    "            \"P-value\": p_value,\n",
    "            \"Rank-Biserial\": r\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    return effect_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277deaa-d472-4f2d-9e85-e58e9a405444",
   "metadata": {},
   "source": [
    "Another option is to compare odds ratios. For this, we need to collapse the ranks into binary \"low\" (1, 2) and \"high\" (3, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e63859-23ec-4a3b-807e-e88eb0a293bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_ratios(group_1, group_2, column):\n",
    "    # Collapse to binary outcome: high = 3 or 4\n",
    "    group_1[\"High\"] = (group_1[column] >= 3).astype(int)\n",
    "    group_2[\"High\"] = (group_2[column] >= 3).astype(int)\n",
    "\n",
    "    # create 2x2 contingency table\n",
    "    table = pd.DataFrame(\n",
    "        [[len(group_1[group_1['High']==0]), len(group_1[group_1['High']==1])],   \n",
    "         [len(group_2[group_2['High']==0]), len(group_2[group_2['High']==1])]],  \n",
    "        index=[\"before\", \"after\"],\n",
    "        columns=[\"low\", \"high\"]\n",
    "    )\n",
    "    table2x2 = Table2x2(table.values)\n",
    "    \n",
    "    # calculate and print\n",
    "    print(\"\\n\", table)\n",
    "    print(\"Odds ratio:\", table2x2.oddsratio)\n",
    "    print(\"95% CI:\", table2x2.oddsratio_confint())\n",
    "    print(\"P-value (Fisher exact):\", table2x2.test_nominal_association().pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a2a26-7c4a-4868-99f8-4f43dbbf16b3",
   "metadata": {},
   "source": [
    "## Hypothesis A\n",
    "\n",
    "First, let's select the conference and then check the distribution over the years. Then you can specify the years to create the groups, before the dataframe is filtered and the test carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29086837-6ffa-483f-89e9-92525022f0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2017    16\n",
      "2018    17\n",
      "2019    18\n",
      "2020    21\n",
      "2021    13\n",
      "2022    20\n",
      "2023    13\n",
      "2024    12\n",
      "Name: count, dtype: int64 130\n"
     ]
    }
   ],
   "source": [
    "df_A = df_nocp[df_nocp[CONF_COL] == 'agile']\n",
    "\n",
    "print(df_A['year'].value_counts().sort_index(), len(df_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f40d7fb-f5c9-460d-8708-48504c283e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 58\n"
     ]
    }
   ],
   "source": [
    "# specify the years for each group\n",
    "GROUP_A1 = [2017, 2018, 2019]\n",
    "GROUP_A2 = [2021, 2022, 2023,2024]\n",
    "\n",
    "# Create the two groups\n",
    "group_a1 = df_A[df_A[YEAR_COL].isin(GROUP_A1)][TEST_COL].dropna()\n",
    "group_a2 = df_A[df_A[YEAR_COL].isin(GROUP_A2)][TEST_COL].dropna()\n",
    "\n",
    "print(len(group_a1), len(group_a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6ebd590-218a-4f20-8db7-b742df89a671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_38135\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_38135_level0_col0\" class=\"col_heading level0 col0\" >Group</th>\n",
       "      <th id=\"T_38135_level0_col1\" class=\"col_heading level0 col1\" >criterion</th>\n",
       "      <th id=\"T_38135_level0_col2\" class=\"col_heading level0 col2\" >mean</th>\n",
       "      <th id=\"T_38135_level0_col3\" class=\"col_heading level0 col3\" >mode</th>\n",
       "      <th id=\"T_38135_level0_col4\" class=\"col_heading level0 col4\" >median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row0_col0\" class=\"data row0 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_38135_row0_col1\" class=\"data row0 col1\" >consolidated_data</td>\n",
       "      <td id=\"T_38135_row0_col2\" class=\"data row0 col2\" >0.745098</td>\n",
       "      <td id=\"T_38135_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_38135_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row1_col0\" class=\"data row1 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_38135_row1_col1\" class=\"data row1 col1\" >consolidated_data</td>\n",
       "      <td id=\"T_38135_row1_col2\" class=\"data row1 col2\" >1.586207</td>\n",
       "      <td id=\"T_38135_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "      <td id=\"T_38135_row1_col4\" class=\"data row1 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row2_col0\" class=\"data row2 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_38135_row2_col1\" class=\"data row2 col1\" >consolidated_methods</td>\n",
       "      <td id=\"T_38135_row2_col2\" class=\"data row2 col2\" >1.019608</td>\n",
       "      <td id=\"T_38135_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "      <td id=\"T_38135_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row3_col0\" class=\"data row3 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_38135_row3_col1\" class=\"data row3 col1\" >consolidated_methods</td>\n",
       "      <td id=\"T_38135_row3_col2\" class=\"data row3 col2\" >1.931034</td>\n",
       "      <td id=\"T_38135_row3_col3\" class=\"data row3 col3\" >2</td>\n",
       "      <td id=\"T_38135_row3_col4\" class=\"data row3 col4\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row4_col0\" class=\"data row4 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_38135_row4_col1\" class=\"data row4 col1\" >consolidated_results</td>\n",
       "      <td id=\"T_38135_row4_col2\" class=\"data row4 col2\" >1.019608</td>\n",
       "      <td id=\"T_38135_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "      <td id=\"T_38135_row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_38135_row5_col0\" class=\"data row5 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_38135_row5_col1\" class=\"data row5 col1\" >consolidated_results</td>\n",
       "      <td id=\"T_38135_row5_col2\" class=\"data row5 col2\" >1.741379</td>\n",
       "      <td id=\"T_38135_row5_col3\" class=\"data row5 col3\" >1</td>\n",
       "      <td id=\"T_38135_row5_col4\" class=\"data row5 col4\" >2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_20f16\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_20f16_level0_col0\" class=\"col_heading level0 col0\" >Criterion</th>\n",
       "      <th id=\"T_20f16_level0_col1\" class=\"col_heading level0 col1\" >U-Statistic</th>\n",
       "      <th id=\"T_20f16_level0_col2\" class=\"col_heading level0 col2\" >P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_20f16_row0_col0\" class=\"data row0 col0\" >consolidated_data</td>\n",
       "      <td id=\"T_20f16_row0_col1\" class=\"data row0 col1\" >804.000000</td>\n",
       "      <td id=\"T_20f16_row0_col2\" class=\"data row0 col2\" >0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_20f16_row1_col0\" class=\"data row1 col0\" >consolidated_methods</td>\n",
       "      <td id=\"T_20f16_row1_col1\" class=\"data row1 col1\" >565.000000</td>\n",
       "      <td id=\"T_20f16_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_20f16_row2_col0\" class=\"data row2 col0\" >consolidated_results</td>\n",
       "      <td id=\"T_20f16_row2_col1\" class=\"data row2 col1\" >740.000000</td>\n",
       "      <td id=\"T_20f16_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_stats = pd.DataFrame()\n",
    "u_stats = pd.DataFrame()\n",
    "\n",
    "for column in TEST_COL:\n",
    "    d_stats = pd.concat([d_stats, descr_stats(group_a1, group_a2, column)], ignore_index=True)\n",
    "    u_stats = pd.concat([u_stats, mann_whitney_u(group_a1, group_a2, column)], ignore_index=True)\n",
    "\n",
    "display(d_stats.style.hide(axis=\"index\"))\n",
    "display(u_stats.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afda5e-7f2b-4004-aaf4-ba9babf6076c",
   "metadata": {},
   "source": [
    "For all three dimensions, we observe a statistically significant increase in potential reproducibility at the chosen significance level of 0.01.\n",
    "\n",
    "We therefore reject the original hypothesis for data, methods, and results dimensions.\n",
    "\n",
    "Further, the descriptive statistics show that for all dimension/statistic combinations (except the mode of results) the ranks have improved. Especially the increase of mode and medians for data and methods is meaningful, because improving to rank 2 (\"Available\") has the largest practical impact. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf31c5-5f40-4ef9-be94-11af314650c9",
   "metadata": {},
   "source": [
    "## Hypothesis B\n",
    "\n",
    "Follows the pattern of Hypothesis a: First, let's select the conference and then check the distribution over the years. Then you can specify the years to create the groups, before the dataframe is filtered and the test carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d65e1581-0c58-4506-b9e0-9ccd4c4bb144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year\n",
      "2016    17\n",
      "2018    17\n",
      "2020    15\n",
      "2021    13\n",
      "2023    11\n",
      "Name: count, dtype: int64 73\n"
     ]
    }
   ],
   "source": [
    "df_B = df_nocp[df_nocp[CONF_COL] == 'giscience']\n",
    "\n",
    "print(df_B['year'].value_counts().sort_index(), len(df_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8674a5b4-7527-429c-a9f7-a620e1f29ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 24\n"
     ]
    }
   ],
   "source": [
    "# specify the years for each group\n",
    "GROUP_B1 = [2016, 2018]\n",
    "GROUP_B2 = [2021, 2023]\n",
    "\n",
    "# Create the two groups\n",
    "group_b1 = df_B[df_B[YEAR_COL].isin(GROUP_B1)][TEST_COL].dropna()\n",
    "group_b2 = df_B[df_B[YEAR_COL].isin(GROUP_B2)][TEST_COL].dropna()\n",
    "\n",
    "print(len(group_b1), len(group_b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de5c9941-9367-41b1-a876-251d3f3beb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e4c0a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e4c0a_level0_col0\" class=\"col_heading level0 col0\" >Group</th>\n",
       "      <th id=\"T_e4c0a_level0_col1\" class=\"col_heading level0 col1\" >criterion</th>\n",
       "      <th id=\"T_e4c0a_level0_col2\" class=\"col_heading level0 col2\" >mean</th>\n",
       "      <th id=\"T_e4c0a_level0_col3\" class=\"col_heading level0 col3\" >mode</th>\n",
       "      <th id=\"T_e4c0a_level0_col4\" class=\"col_heading level0 col4\" >median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row0_col0\" class=\"data row0 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row0_col1\" class=\"data row0 col1\" >consolidated_data</td>\n",
       "      <td id=\"T_e4c0a_row0_col2\" class=\"data row0 col2\" >0.647059</td>\n",
       "      <td id=\"T_e4c0a_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_e4c0a_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row1_col0\" class=\"data row1 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row1_col1\" class=\"data row1 col1\" >consolidated_data</td>\n",
       "      <td id=\"T_e4c0a_row1_col2\" class=\"data row1 col2\" >0.750000</td>\n",
       "      <td id=\"T_e4c0a_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_e4c0a_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row2_col0\" class=\"data row2 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row2_col1\" class=\"data row2 col1\" >consolidated_methods</td>\n",
       "      <td id=\"T_e4c0a_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_e4c0a_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "      <td id=\"T_e4c0a_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row3_col0\" class=\"data row3 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row3_col1\" class=\"data row3 col1\" >consolidated_methods</td>\n",
       "      <td id=\"T_e4c0a_row3_col2\" class=\"data row3 col2\" >1.500000</td>\n",
       "      <td id=\"T_e4c0a_row3_col3\" class=\"data row3 col3\" >1</td>\n",
       "      <td id=\"T_e4c0a_row3_col4\" class=\"data row3 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row4_col0\" class=\"data row4 col0\" >Pre-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row4_col1\" class=\"data row4 col1\" >consolidated_results</td>\n",
       "      <td id=\"T_e4c0a_row4_col2\" class=\"data row4 col2\" >0.941176</td>\n",
       "      <td id=\"T_e4c0a_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "      <td id=\"T_e4c0a_row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e4c0a_row5_col0\" class=\"data row5 col0\" >Post-Intervention</td>\n",
       "      <td id=\"T_e4c0a_row5_col1\" class=\"data row5 col1\" >consolidated_results</td>\n",
       "      <td id=\"T_e4c0a_row5_col2\" class=\"data row5 col2\" >1.333333</td>\n",
       "      <td id=\"T_e4c0a_row5_col3\" class=\"data row5 col3\" >1</td>\n",
       "      <td id=\"T_e4c0a_row5_col4\" class=\"data row5 col4\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_72d26\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_72d26_level0_col0\" class=\"col_heading level0 col0\" >Criterion</th>\n",
       "      <th id=\"T_72d26_level0_col1\" class=\"col_heading level0 col1\" >U-Statistic</th>\n",
       "      <th id=\"T_72d26_level0_col2\" class=\"col_heading level0 col2\" >P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_72d26_row0_col0\" class=\"data row0 col0\" >consolidated_data</td>\n",
       "      <td id=\"T_72d26_row0_col1\" class=\"data row0 col1\" >389.000000</td>\n",
       "      <td id=\"T_72d26_row0_col2\" class=\"data row0 col2\" >0.747457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_72d26_row1_col0\" class=\"data row1 col0\" >consolidated_methods</td>\n",
       "      <td id=\"T_72d26_row1_col1\" class=\"data row1 col1\" >242.000000</td>\n",
       "      <td id=\"T_72d26_row1_col2\" class=\"data row1 col2\" >0.000471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_72d26_row2_col0\" class=\"data row2 col0\" >consolidated_results</td>\n",
       "      <td id=\"T_72d26_row2_col1\" class=\"data row2 col1\" >288.000000</td>\n",
       "      <td id=\"T_72d26_row2_col2\" class=\"data row2 col2\" >0.001641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_stats = pd.DataFrame()\n",
    "u_stats = pd.DataFrame()\n",
    "\n",
    "for column in TEST_COL:\n",
    "    d_stats = pd.concat([d_stats, descr_stats(group_b1, group_b2, column)], ignore_index=True)\n",
    "    u_stats = pd.concat([u_stats, mann_whitney_u(group_b1, group_b2, column)], ignore_index=True)\n",
    "\n",
    "display(d_stats.style.hide(axis=\"index\"))\n",
    "display(u_stats.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b8681-9f03-4fe8-973b-6e5c50b6e95f",
   "metadata": {},
   "source": [
    "For data, there was no statistically significant increase in potential reproducibility at the chosen significance level of 0.01.\n",
    "\n",
    "For methods and results, there were statistically significant increases in potential reproducibilty at the chosen significance level of 0.01.\n",
    "\n",
    "We therefore reject the original hypothesis only for the data dimension. \n",
    "\n",
    "While the mean ranks have increased for all dimensions, the mode and media stayed the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbde453-046e-46c6-98a1-5dda22673e75",
   "metadata": {},
   "source": [
    "## Hypothesis C\n",
    "\n",
    "This step relies on the previous steps to build the groups for the testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f060a48a-74c7-404e-80e5-a6fae56aea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First the effect sizes for the pre-/post-intervention groups from Hypothesis A:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bf113\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_bf113_level0_col0\" class=\"col_heading level0 col0\" >Criterion</th>\n",
       "      <th id=\"T_bf113_level0_col1\" class=\"col_heading level0 col1\" >U-Statistic</th>\n",
       "      <th id=\"T_bf113_level0_col2\" class=\"col_heading level0 col2\" >P-value</th>\n",
       "      <th id=\"T_bf113_level0_col3\" class=\"col_heading level0 col3\" >Rank-Biserial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_bf113_row0_col0\" class=\"data row0 col0\" >consolidated_data</td>\n",
       "      <td id=\"T_bf113_row0_col1\" class=\"data row0 col1\" >804.000000</td>\n",
       "      <td id=\"T_bf113_row0_col2\" class=\"data row0 col2\" >0.000020</td>\n",
       "      <td id=\"T_bf113_row0_col3\" class=\"data row0 col3\" >0.456389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf113_row1_col0\" class=\"data row1 col0\" >consolidated_methods</td>\n",
       "      <td id=\"T_bf113_row1_col1\" class=\"data row1 col1\" >565.000000</td>\n",
       "      <td id=\"T_bf113_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "      <td id=\"T_bf113_row1_col3\" class=\"data row1 col3\" >0.617985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf113_row2_col0\" class=\"data row2 col0\" >consolidated_results</td>\n",
       "      <td id=\"T_bf113_row2_col1\" class=\"data row2 col1\" >740.000000</td>\n",
       "      <td id=\"T_bf113_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "      <td id=\"T_bf113_row2_col3\" class=\"data row2 col3\" >0.499662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Then the effect sizes for the pre-/post-intervention groups from Hypothesis B:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d2397\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d2397_level0_col0\" class=\"col_heading level0 col0\" >Criterion</th>\n",
       "      <th id=\"T_d2397_level0_col1\" class=\"col_heading level0 col1\" >U-Statistic</th>\n",
       "      <th id=\"T_d2397_level0_col2\" class=\"col_heading level0 col2\" >P-value</th>\n",
       "      <th id=\"T_d2397_level0_col3\" class=\"col_heading level0 col3\" >Rank-Biserial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d2397_row0_col0\" class=\"data row0 col0\" >consolidated_data</td>\n",
       "      <td id=\"T_d2397_row0_col1\" class=\"data row0 col1\" >389.000000</td>\n",
       "      <td id=\"T_d2397_row0_col2\" class=\"data row0 col2\" >0.747457</td>\n",
       "      <td id=\"T_d2397_row0_col3\" class=\"data row0 col3\" >0.046569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d2397_row1_col0\" class=\"data row1 col0\" >consolidated_methods</td>\n",
       "      <td id=\"T_d2397_row1_col1\" class=\"data row1 col1\" >242.000000</td>\n",
       "      <td id=\"T_d2397_row1_col2\" class=\"data row1 col2\" >0.000471</td>\n",
       "      <td id=\"T_d2397_row1_col3\" class=\"data row1 col3\" >0.406863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d2397_row2_col0\" class=\"data row2 col0\" >consolidated_results</td>\n",
       "      <td id=\"T_d2397_row2_col1\" class=\"data row2 col1\" >288.000000</td>\n",
       "      <td id=\"T_d2397_row2_col2\" class=\"data row2 col2\" >0.001641</td>\n",
       "      <td id=\"T_d2397_row2_col3\" class=\"data row2 col3\" >0.294118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x263ad6f22c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "effect_stats_a = pd.DataFrame()\n",
    "\n",
    "print(\"\\nFirst the effect sizes for the pre-/post-intervention groups from Hypothesis A:\")\n",
    "\n",
    "for column in TEST_COL:\n",
    "    effect_stats_a = pd.concat([effect_stats_a, rank_biserial(group_a1, group_a2, column)], ignore_index=True)\n",
    "\n",
    "display(effect_stats_a.style.hide(axis=\"index\"))\n",
    "\n",
    "print(\"\\nThen the effect sizes for the pre-/post-intervention groups from Hypothesis B:\")\n",
    "\n",
    "effect_stats_b = pd.DataFrame()\n",
    "\n",
    "for column in TEST_COL:\n",
    "    effect_stats_b = pd.concat([effect_stats_b, rank_biserial(group_b1, group_b2, column)], ignore_index=True)\n",
    "\n",
    "display(effect_stats_b.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71785d8-e5ad-44aa-aa5c-1d63f69a6cd7",
   "metadata": {},
   "source": [
    "For every tested dimension, the effect size for conference A (AGILE) is larger than for conference B (GIScience). This supports the argument that\n",
    "\n",
    "- there has been a general increase in potential reproducibility in the research domain over the study period\n",
    "- the increase is larger in relative terms and absolute ranks for AGILE\n",
    "\n",
    "Whether this is due to the introduction of the guidelines and the reproducibility reviews is open for interpretation. It should also be noted that there is an overlap in the community (in terms of authors, reviewers, and scientific program committee), so one could expect a certain type of informal spill-over effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297fc91-0f10-43b9-ab81-5d28d0b40c5b",
   "metadata": {},
   "source": [
    "Finally, we can also compare the odds ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "640a2abf-7724-4b2e-a079-2b4d0f690f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column: consolidated_data\n",
      "\n",
      "Group A:\n",
      "\n",
      "         low  high\n",
      "before   51     0\n",
      "after    46    12\n",
      "Odds ratio: 26.608695652173914\n",
      "95% CI: (np.float64(1.5285797936024483), np.float64(463.1898755127469))\n",
      "P-value (Fisher exact): 0.0012009731256340528\n",
      "\n",
      "Group B:\n",
      "\n",
      "         low  high\n",
      "before   34     0\n",
      "after    22     2\n",
      "Odds ratio: 6.181818181818182\n",
      "95% CI: (np.float64(0.2662226714666457), np.float64(143.54478460654204))\n",
      "P-value (Fisher exact): 0.2003755421490082\n",
      "\n",
      "Column: consolidated_methods\n",
      "\n",
      "Group A:\n",
      "\n",
      "         low  high\n",
      "before   51     0\n",
      "after    43    15\n",
      "Odds ratio: 35.58139534883721\n",
      "95% CI: (np.float64(2.0649946685469067), np.float64(613.0939291291904))\n",
      "P-value (Fisher exact): 0.0001919252787526693\n",
      "\n",
      "Group B:\n",
      "\n",
      "         low  high\n",
      "before   34     0\n",
      "after    22     2\n",
      "Odds ratio: 6.181818181818182\n",
      "95% CI: (np.float64(0.2662226714666457), np.float64(143.54478460654204))\n",
      "P-value (Fisher exact): 0.2003755421490082\n",
      "\n",
      "Column: consolidated_results\n",
      "\n",
      "Group A:\n",
      "\n",
      "         low  high\n",
      "before   51     0\n",
      "after    46    12\n",
      "Odds ratio: 26.608695652173914\n",
      "95% CI: (np.float64(1.5285797936024483), np.float64(463.1898755127469))\n",
      "P-value (Fisher exact): 0.0012009731256340528\n",
      "\n",
      "Group B:\n",
      "\n",
      "         low  high\n",
      "before   34     0\n",
      "after    22     2\n",
      "Odds ratio: 6.181818181818182\n",
      "95% CI: (np.float64(0.2662226714666457), np.float64(143.54478460654204))\n",
      "P-value (Fisher exact): 0.2003755421490082\n"
     ]
    }
   ],
   "source": [
    "for column in TEST_COL:\n",
    "    print(\"\\nColumn: \" + column)\n",
    "    print(\"\\nGroup A:\")\n",
    "    odds_ratios(group_a1, group_a2, column)\n",
    "    print(\"\\nGroup B:\")\n",
    "    odds_ratios(group_b1, group_b2, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7a726-9133-4822-b35c-9f313ed16caa",
   "metadata": {},
   "source": [
    "For all reproducibility dimensions, the odds rarios for an improvement are higher for AGILE than for GScience, and all have a p-Value below our chosen significance level of 0.01 for AGILE, while none have for GIScience. This is another indication that the improvement for AGILE has been stronger than for GIScience. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
